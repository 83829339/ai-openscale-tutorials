{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run using with Python 3.x with Spark runtime environment. If you are viewing this in Watson Studio and do not see Python 3.x with Spark in the upper right corner of your screen, please update the runtime now. It requires service credentials for the following services:\n",
    "\n",
    "<li>Watson OpenScale\n",
    "<li>Watson Machine Learning\n",
    "<li>DB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials for IBM Cloud Pak for Data\n",
    "In the following code boxes, you need to replace the sample data with your own credentials. You can acquire the information from your system administrator or through the Cloud Pak for Data dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining your Watson OpenScale credentials\n",
    "You can retrieve the URL by running the following command: `oc get route -n namespace1 --no-headers | awk '{print $2}'`\n",
    "Replace the `namespace1` variable with your namespace.\n",
    "\n",
    "You should have been assigned a username and password when you were added to the Cloud Pak for Data system. You might need to ask either your database administrator or your system administrator for some of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOS_CREDENTIALS = \"YOUR_WOS_CREDENTIALS_HERE\"\n",
    "\n",
    "#####################################\n",
    "# Example credentials will look below\n",
    "#####################################\n",
    "\n",
    "# WOS_CREDENTIALS = {\n",
    "#     \"url\": \"https://CLUSTER_NAME\",\n",
    "#     \"username\": \"admin\",\n",
    "#     \"password\": \"password\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Watson OpenScale GUID\n",
    "For most systems, the default GUID is already entered for you. You would only need to update this particular entry if the GUID was changed from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOS_GUID=\"00000000-0000-0000-0000-000000000000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Watson OpenScale GUID\n",
    "For most systems, the default GUID is already entered for you. You would only need to update this particular entry if the GUID was changed from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = WOS_CREDENTIALS.copy()\n",
    "WML_CREDENTIALS['instance_id']='openshift'\n",
    "WML_CREDENTIALS['version']='2.5.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your database credentials and schema\n",
    "Normally, you must obtain the database credentials and schema name from your database administrator, however, if you deployed one of the database options from the Cloud Pak for Data UI, you should be able to retrieve credentials yourself by going to My data and right-clicking on the database tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_CREDENTIALS = \"YOUR_DB_CREDENTIALS_HERE\"\n",
    "\n",
    "#####################################\n",
    "# Example credentials will look below\n",
    "#####################################\n",
    "\n",
    "# DATABASE_CREDENTIALS = {\n",
    "#     \"db\": \"SAMPLE\",\n",
    "#     \"db_type\": \"db2\",\n",
    "#     \"hostname\": \"hostname\",\n",
    "#     \"password\": \"password\",\n",
    "#     \"port\": 50000,\n",
    "#     \"username\": \"db2inst1\"\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"YOUR_SCHEMA_NAME_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration system credentials (IBM OpenPages MRG) \n",
    "As part of the closed beta, you were sent an email message from IBM that provides the URL, username, and password of an IBM OpenPages system that you can use. Replace the following credentials with the ones that you received in that email message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENPAGES_CREDENTIALS = \"OPENPAGES_CREDENTIALS_HERE\"\n",
    "\n",
    "#####################################\n",
    "# Example credentials will look below\n",
    "#####################################\n",
    "\n",
    "# OPENPAGES_CREDENTIALS = {\n",
    "#    \"url\": \"https://softlayer-dev1.op-ibm.com\",\n",
    "#    \"username\": \"username\",\n",
    "#    \"password\": \"password\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "The following opensource packages must be installed into this notebook instance so that they are available to use during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/spark/shared/user-libs/python3.6*\n",
    "\n",
    "!pip install pyspark==2.3 --no-cache | tail -n 1\n",
    "!pip install numpy==1.15.4 --no-cache | tail -n 1\n",
    "!pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1\n",
    "!pip install --upgrade SciPy --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data from Github\n",
    "So you don't have to manually generate training data, we've provided a sample and placed it in a publicly available Github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm german_credit_data_biased_training.csv\n",
    "!wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/german_credit_data_biased_training.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an OpenPages Model\n",
    "\n",
    "The following cells creates and OpenPages GRC Object (Model) that we will use as an integrated system reference to which we will publish the OpenScale metrics generated as a part of the MRM risk evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import base64\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"application/json\"\n",
    "auth = HTTPBasicAuth(OPENPAGES_CREDENTIALS[\"username\"], OPENPAGES_CREDENTIALS[\"password\"])\n",
    "\n",
    "GET_OPENPAGES_MODEL_DEFINITION_URL = OPENPAGES_CREDENTIALS[\"url\"] + \"/grc/api/types/Model\"\n",
    "\n",
    "response = requests.get(GET_OPENPAGES_MODEL_DEFINITION_URL, headers=headers, auth=auth)\n",
    "json_data = response.json()\n",
    "\n",
    "if \"id\" in json_data:\n",
    "    openpages_model_definition_id = json_data[\"id\"]\n",
    "\n",
    "if \"fieldDefinitions\" in json_data:\n",
    "    for fieldDefinition in json_data[\"fieldDefinitions\"][\"fieldDefinition\"]:\n",
    "        if fieldDefinition[\"name\"] == \"MRG-Model:Model Owner\":\n",
    "            model_owner_field_definition_id = fieldDefinition[\"id\"]\n",
    "        elif fieldDefinition[\"name\"] == \"MRG-Model:Model Non Model\":\n",
    "            model_non_model_field_definition_id = fieldDefinition[\"id\"]\n",
    "        elif fieldDefinition[\"name\"] == \"MRG-Model:Machine Learning Model\":\n",
    "            machine_learning_model_field_definition_id = fieldDefinition[\"id\"]\n",
    "        elif fieldDefinition[\"name\"] == \"MRG-Model:OpenScale Monitor\":\n",
    "            openscale_monitor_field_definition_id = fieldDefinition[\"id\"]\n",
    "        elif fieldDefinition[\"name\"] == \"Description\":\n",
    "            description_field_definition_id = fieldDefinition[\"id\"]\n",
    "        elif fieldDefinition[\"name\"] == \"MRG-Model:Status\":\n",
    "            model_status_field_definition_id = fieldDefinition[\"id\"]\n",
    "        elif fieldDefinition[\"name\"] == \"MRG-Model:Candidate Status\":\n",
    "            model_candidate_status_field_definition_id = fieldDefinition[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"application/json\"\n",
    "auth = HTTPBasicAuth(OPENPAGES_CREDENTIALS[\"username\"], OPENPAGES_CREDENTIALS[\"password\"])\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"OpenPages model created by the MRM E2E notebook for Credit Risk Model\",\n",
    "  \"fields\": {\n",
    "    \"field\": [\n",
    "      {\n",
    "        \"id\": model_owner_field_definition_id,\n",
    "        \"name\": \"MRG-Model:Model Owner\",\n",
    "        \"dataType\": \"STRING_TYPE\",\n",
    "        \"value\": OPENPAGES_CREDENTIALS[\"username\"]\n",
    "      },\n",
    "      {\n",
    "        \"id\": description_field_definition_id,\n",
    "        \"name\": \"Description\",\n",
    "        \"dataType\": \"STRING_TYPE\",\n",
    "        \"value\": \"OpenPages model for Watson OpenScale MRM Demo\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": model_non_model_field_definition_id,\n",
    "        \"name\": \"MRG-Model:Model Non Model\",\n",
    "        \"dataType\": \"ENUM_TYPE\",\n",
    "        \"enumValue\": {\n",
    "          \"name\": \"Model\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"id\": machine_learning_model_field_definition_id,\n",
    "        \"name\": \"MRG-Model:Machine Learning Model\",\n",
    "        \"dataType\": \"ENUM_TYPE\",\n",
    "        \"enumValue\": {\n",
    "          \"name\": \"Yes\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"id\": openscale_monitor_field_definition_id,\n",
    "        \"name\": \"MRG-Model:OpenScale Monitor\",\n",
    "        \"dataType\": \"ENUM_TYPE\",\n",
    "        \"enumValue\": {\n",
    "          \"name\": \"Yes\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"id\": model_status_field_definition_id,\n",
    "        \"name\": \"MRG-Model:Model Status\",\n",
    "        \"dataType\": \"ENUM_TYPE\",\n",
    "        \"enumValue\": {\n",
    "          \"name\": \"Under Development\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"id\": model_candidate_status_field_definition_id,\n",
    "        \"name\": \"MRG-Model:Candidate Status\",\n",
    "        \"dataType\": \"ENUM_TYPE\",\n",
    "        \"enumValue\": {\n",
    "          \"name\": \"Confirmed\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"typeDefinitionId\": openpages_model_definition_id\n",
    "}\n",
    "\n",
    "CREATE_OPENPAGES_MODEL_URL = OPENPAGES_CREDENTIALS[\"url\"] + \"/grc/api/contents\"\n",
    "response = requests.post(CREATE_OPENPAGES_MODEL_URL, json=payload, headers=headers, auth=auth)\n",
    "json_data = response.json()\n",
    "\n",
    "openpages_model_name=json_data[\"name\"]\n",
    "openpages_model_id=json_data[\"id\"]\n",
    "\n",
    "print(\"OpenPages Model Name = {}\".format(openpages_model_name))\n",
    "print(\"OpenPages Model Id = {}\".format(openpages_model_id))\n",
    "print(\"OpenPages URL to view the model = {0}{1}{2}\".format(OPENPAGES_CREDENTIALS[\"url\"], \"/app/jspview/react/grc/task-view/\", openpages_model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the models\n",
    "\n",
    "The following cells deploy both the pre-prod and challenger models into the Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PROD_MODEL_NAME=\"German Credit Risk Model - PreProd\"\n",
    "PRE_PROD_DEPLOYMENT_NAME=\"German Credit Risk Model - PreProd\"\n",
    "\n",
    "PRE_PROD_CHALLENGER_MODEL_NAME=\"German Credit Risk Model - Challenger\"\n",
    "PRE_PROD_CHALLENGER_DEPLOYMENT_NAME=\"German Credit Risk Model - Challenger\"\n",
    "\n",
    "PRE_PROD_SPACE_NAME=\"pre-prod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize WML client and set the space_id parameter\n",
    "Analytic spaces are used to categorize assets and can be associated with a project inside of Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "wml_client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)\n",
    "print(wml_client.service_instance.get_url())\n",
    "\n",
    "wml_client.spaces.list()\n",
    "\n",
    "# Find and set the default space\n",
    "space_name=PRE_PROD_SPACE_NAME\n",
    "spaces = wml_client.spaces.get_details()['resources']\n",
    "space_id = None\n",
    "for space in spaces:\n",
    "    if space['entity']['name'] == space_name:\n",
    "        space_id = space[\"metadata\"][\"guid\"]\n",
    "if space_id is None:\n",
    "    space_id = wml_client.spaces.store(\n",
    "        meta_props={wml_client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]\n",
    "wml_client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Spark Credit Risk Model to WML\n",
    "\n",
    "The following cell deploys the Spark version of the German Credit Risk Model to the specified Machine Learning instance in the specified deployment space. You'll notice that this version of the German Credit Risk model has an auc-roc score around 71%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "numpy.version.version\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier,GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType, ArrayType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "pd_data = pd.read_csv(\"german_credit_data_biased_training.csv\", sep=\",\", header=0)\n",
    "spark_df = spark.read.csv(path=\"german_credit_data_biased_training.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "spark_df.head()\n",
    "\n",
    "(train_data, test_data) = spark_df.randomSplit([0.9, 0.1], 24)\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))\n",
    "\n",
    "si_CheckingStatus = StringIndexer(inputCol='CheckingStatus', outputCol='CheckingStatus_IX')\n",
    "si_CreditHistory = StringIndexer(inputCol='CreditHistory', outputCol='CreditHistory_IX')\n",
    "si_LoanPurpose = StringIndexer(inputCol='LoanPurpose', outputCol='LoanPurpose_IX')\n",
    "si_ExistingSavings = StringIndexer(inputCol='ExistingSavings', outputCol='ExistingSavings_IX')\n",
    "si_EmploymentDuration = StringIndexer(inputCol='EmploymentDuration', outputCol='EmploymentDuration_IX')\n",
    "si_Sex = StringIndexer(inputCol='Sex', outputCol='Sex_IX')\n",
    "si_OthersOnLoan = StringIndexer(inputCol='OthersOnLoan', outputCol='OthersOnLoan_IX')\n",
    "si_OwnsProperty = StringIndexer(inputCol='OwnsProperty', outputCol='OwnsProperty_IX')\n",
    "si_InstallmentPlans = StringIndexer(inputCol='InstallmentPlans', outputCol='InstallmentPlans_IX')\n",
    "si_Housing = StringIndexer(inputCol='Housing', outputCol='Housing_IX')\n",
    "si_Job = StringIndexer(inputCol='Job', outputCol='Job_IX')\n",
    "si_Telephone = StringIndexer(inputCol='Telephone', outputCol='Telephone_IX')\n",
    "si_ForeignWorker = StringIndexer(inputCol='ForeignWorker', outputCol='ForeignWorker_IX')\n",
    "si_Label = StringIndexer(inputCol=\"Risk\", outputCol=\"label\").fit(spark_df)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)\n",
    "\n",
    "va_features = VectorAssembler(\n",
    "inputCols=[\"CheckingStatus_IX\", \"CreditHistory_IX\", \"LoanPurpose_IX\", \"ExistingSavings_IX\",\n",
    "           \"EmploymentDuration_IX\", \"Sex_IX\", \"OthersOnLoan_IX\", \"OwnsProperty_IX\", \"InstallmentPlans_IX\",\n",
    "           \"Housing_IX\", \"Job_IX\", \"Telephone_IX\", \"ForeignWorker_IX\", \"LoanDuration\", \"LoanAmount\",\n",
    "           \"InstallmentPercent\", \"CurrentResidenceDuration\", \"LoanDuration\", \"Age\", \"ExistingCreditsCount\",\n",
    "           \"Dependents\"], outputCol=\"features\")\n",
    "\n",
    "classifier=GBTClassifier(featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "stages=[si_CheckingStatus, si_CreditHistory, si_EmploymentDuration, si_ExistingSavings, si_ForeignWorker,\n",
    "        si_Housing, si_InstallmentPlans, si_Job, si_LoanPurpose, si_OthersOnLoan,\n",
    "        si_OwnsProperty, si_Sex, si_Telephone, si_Label, va_features, classifier, label_converter])\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy = %g\" % auc)\n",
    "\n",
    "# Remove existing model and deployment\n",
    "MODEL_NAME=PRE_PROD_MODEL_NAME\n",
    "DEPLOYMENT_NAME=PRE_PROD_DEPLOYMENT_NAME\n",
    "\n",
    "deployment_details = wml_client.deployments.get_details()\n",
    "for deployment in deployment_details['resources']:\n",
    "    deployment_id = deployment['metadata']['guid']\n",
    "    model_id = deployment['entity']['asset']['href'].split('/')[3].split('?')[0]\n",
    "    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n",
    "        print('Deleting deployment id', deployment_id)\n",
    "        wml_client.deployments.delete(deployment_id)\n",
    "        print('Deleting model id', model_id)\n",
    "        wml_client.repository.delete(model_id)\n",
    "wml_client.repository.list_models()\n",
    "\n",
    "# Save Model\n",
    "model_props_rf = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.DESCRIPTION: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.RUNTIME_UID: \"spark-mllib_2.3\",\n",
    "    wml_client.repository.ModelMetaNames.TYPE: \"mllib_2.3\"\n",
    "}\n",
    "\n",
    "published_model_details = wml_client.repository.store_model(model=model, meta_props=model_props_rf, training_data=train_data, pipeline=pipeline)\n",
    "print(published_model_details)\n",
    "\n",
    "# List models in the repository\n",
    "wml_client.repository.list_models()\n",
    "\n",
    "# Get the model UID\n",
    "pre_prod_model_uid = wml_client.repository.get_model_uid(published_model_details)\n",
    "pre_prod_model_uid\n",
    "\n",
    "\n",
    "# Deploy model\n",
    "wml_deployments = wml_client.deployments.get_details()\n",
    "pre_prod_deployment_uid = None\n",
    "for deployment in wml_deployments['resources']:\n",
    "    if DEPLOYMENT_NAME == deployment['entity']['name']:\n",
    "        pre_prod_deployment_uid = deployment['metadata']['guid']\n",
    "        break\n",
    "\n",
    "if pre_prod_deployment_uid is None:\n",
    "    print(\"Deploying model...\")\n",
    "    meta_props = {\n",
    "        wml_client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n",
    "        wml_client.deployments.ConfigurationMetaNames.DESCRIPTION: DEPLOYMENT_NAME,\n",
    "        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "    deployment = wml_client.deployments.create(artifact_uid=pre_prod_model_uid, name=DEPLOYMENT_NAME, meta_props=meta_props)\n",
    "    pre_prod_deployment_uid = wml_client.deployments.get_uid(deployment)\n",
    "\n",
    "print(\"Model id: {}\".format(pre_prod_model_uid))\n",
    "print(\"Deployment id: {}\".format(pre_prod_deployment_uid))\n",
    "\n",
    "pre_prod_deployment_uid=wml_client.deployments.get_uid(deployment)\n",
    "pre_prod_deployment_uid\n",
    "\n",
    "fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"]\n",
    "values = [\n",
    "  [\"no_checking\",13,\"credits_paid_to_date\",\"car_new\",1343,\"100_to_500\",\"1_to_4\",2,\"female\",\"none\",3,\"savings_insurance\",46,\"none\",\"own\",2,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",24,\"prior_payments_delayed\",\"furniture\",4567,\"500_to_1000\",\"1_to_4\",4,\"male\",\"none\",4,\"savings_insurance\",36,\"none\",\"free\",2,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",26,\"all_credits_paid_back\",\"car_new\",863,\"less_100\",\"less_1\",2,\"female\",\"co-applicant\",2,\"real_estate\",38,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",14,\"no_credits\",\"car_new\",2368,\"less_100\",\"1_to_4\",3,\"female\",\"none\",3,\"real_estate\",29,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",4,\"no_credits\",\"car_new\",250,\"less_100\",\"unemployed\",2,\"female\",\"none\",3,\"real_estate\",23,\"none\",\"rent\",1,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",17,\"credits_paid_to_date\",\"car_new\",832,\"100_to_500\",\"1_to_4\",2,\"male\",\"none\",2,\"real_estate\",42,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",33,\"outstanding_credit\",\"appliances\",5696,\"unknown\",\"greater_7\",4,\"male\",\"co-applicant\",4,\"unknown\",54,\"none\",\"free\",2,\"skilled\",1,\"yes\",\"yes\"],\n",
    "  [\"0_to_200\",13,\"prior_payments_delayed\",\"retraining\",1375,\"100_to_500\",\"4_to_7\",3,\"male\",\"none\",3,\"real_estate\",37,\"none\",\"own\",2,\"management_self-employed\",1,\"none\",\"yes\"]\n",
    "]\n",
    "\n",
    "payload_scoring = {\"fields\": fields,\"values\": values}\n",
    "payload = {\n",
    "    wml_client.deployments.ScoringMetaNames.INPUT_DATA: [payload_scoring]\n",
    "}\n",
    "scoring_response = wml_client.deployments.score(pre_prod_deployment_uid, payload)\n",
    "\n",
    "scoring_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Scikit-Learn Credit Risk Model to Watson Machine Learning\n",
    "\n",
    "The following cell deploys the Scikit-learn version of the German Credit Risk Model to the specified Machine Learning instance in the specified deployment space. This version of the German Credit Risk model has an auc-roc score around 85% and will be called the \"Challenger.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import numpy\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_df=pd.read_csv (\"german_credit_data_biased_training.csv\")\n",
    "\n",
    "data_df.head()\n",
    "\n",
    "target_label_name = \"Risk\"\n",
    "feature_cols= data_df.drop(columns=[target_label_name])\n",
    "label= data_df[target_label_name]\n",
    "\n",
    "# Set model evaluation properties\n",
    "optimization_metric = 'roc_auc'\n",
    "random_state = 33\n",
    "cv_num_folds = 3\n",
    "holdout_fraction = 0.1\n",
    "\n",
    "if type_of_target(label.values) in ['multiclass', 'binary']:\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(feature_cols, label, test_size=holdout_fraction, random_state=random_state, stratify=label.values)\n",
    "else:\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(feature_cols, label, test_size=holdout_fraction, random_state=random_state)\n",
    "\n",
    "# Data preprocessing transformer generation\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('OrdinalEncoder', OrdinalEncoder(categories='auto',dtype=numpy.float64 ))])\n",
    "\n",
    "numeric_features = feature_cols.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = feature_cols.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Initiate model and create pipeline\n",
    "model=sklearn.ensemble.gradient_boosting.GradientBoostingClassifier()\n",
    "gbt_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "model_gbt=gbt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_gbt.predict(X_holdout)\n",
    "\n",
    "\n",
    "# Evaluate model performance on test data and Cross validation\n",
    "scorer = get_scorer(optimization_metric)\n",
    "scorer(model_gbt,X_holdout, y_holdout)\n",
    "\n",
    "# Cross validation -3 folds\n",
    "cv_results = cross_validate(model_gbt,X_train,y_train, scoring={optimization_metric:scorer})\n",
    "numpy.mean(cv_results['test_' + optimization_metric])\n",
    "\n",
    "print(classification_report(y_pred, y_holdout))\n",
    "\n",
    "\n",
    "# Remove existing model and deployment\n",
    "MODEL_NAME=PRE_PROD_CHALLENGER_MODEL_NAME\n",
    "DEPLOYMENT_NAME=PRE_PROD_CHALLENGER_DEPLOYMENT_NAME\n",
    "\n",
    "deployment_details = wml_client.deployments.get_details()\n",
    "for deployment in deployment_details['resources']:\n",
    "    deployment_id = deployment['metadata']['guid']\n",
    "    model_id = deployment['entity']['asset']['href'].split('/')[3].split('?')[0]\n",
    "    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n",
    "        print('Deleting deployment id', deployment_id)\n",
    "        wml_client.deployments.delete(deployment_id)\n",
    "        print('Deleting model id', model_id)\n",
    "        wml_client.repository.delete(model_id)\n",
    "wml_client.repository.list_models()\n",
    "\n",
    "# Store Model\n",
    "model_props_gbt = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.DESCRIPTION: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.RUNTIME_UID: \"scikit-learn_0.20-py3.6\",\n",
    "    wml_client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.20\"\n",
    "}\n",
    "\n",
    "published_model_details = wml_client.repository.store_model(model=model_gbt, meta_props=model_props_gbt, training_data=feature_cols,training_target=label)\n",
    "print(published_model_details)\n",
    "\n",
    "# List models in the repository\n",
    "wml_client.repository.list_models()\n",
    "\n",
    "# Get the model UID\n",
    "challenger_model_uid = wml_client.repository.get_model_uid(published_model_details)\n",
    "challenger_model_uid\n",
    "\n",
    "\n",
    "# Deploy model\n",
    "wml_deployments = wml_client.deployments.get_details()\n",
    "challenger_deployment_uid = None\n",
    "for deployment in wml_deployments['resources']:\n",
    "    if DEPLOYMENT_NAME == deployment['entity']['name']:\n",
    "        challenger_deployment_uid = deployment['metadata']['guid']\n",
    "        break\n",
    "\n",
    "if challenger_deployment_uid is None:\n",
    "    print(\"Deploying model...\")\n",
    "    meta_props = {\n",
    "        wml_client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n",
    "        wml_client.deployments.ConfigurationMetaNames.DESCRIPTION: DEPLOYMENT_NAME,\n",
    "        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "    deployment = wml_client.deployments.create(artifact_uid=challenger_model_uid, meta_props=meta_props)\n",
    "    challenger_deployment_uid = wml_client.deployments.get_uid(deployment)\n",
    "\n",
    "print(\"Model id: {}\".format(challenger_model_uid))\n",
    "print(\"Deployment id: {}\".format(challenger_deployment_uid))\n",
    "\n",
    "challenger_deployment_uid=wml_client.deployments.get_uid(deployment)\n",
    "challenger_deployment_uid\n",
    "\n",
    "\n",
    "# Sample scoring\n",
    "fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"]\n",
    "values = [\n",
    "  [\"no_checking\",13,\"credits_paid_to_date\",\"car_new\",1343,\"100_to_500\",\"1_to_4\",2,\"female\",\"none\",3,\"savings_insurance\",46,\"none\",\"own\",2,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",24,\"prior_payments_delayed\",\"furniture\",4567,\"500_to_1000\",\"1_to_4\",4,\"male\",\"none\",4,\"savings_insurance\",36,\"none\",\"free\",2,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",26,\"all_credits_paid_back\",\"car_new\",863,\"less_100\",\"less_1\",2,\"female\",\"co-applicant\",2,\"real_estate\",38,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",14,\"no_credits\",\"car_new\",2368,\"less_100\",\"1_to_4\",3,\"female\",\"none\",3,\"real_estate\",29,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",4,\"no_credits\",\"car_new\",250,\"less_100\",\"unemployed\",2,\"female\",\"none\",3,\"real_estate\",23,\"none\",\"rent\",1,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",17,\"credits_paid_to_date\",\"car_new\",832,\"100_to_500\",\"1_to_4\",2,\"male\",\"none\",2,\"real_estate\",42,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",33,\"outstanding_credit\",\"appliances\",5696,\"unknown\",\"greater_7\",4,\"male\",\"co-applicant\",4,\"unknown\",54,\"none\",\"free\",2,\"skilled\",1,\"yes\",\"yes\"],\n",
    "  [\"0_to_200\",13,\"prior_payments_delayed\",\"retraining\",1375,\"100_to_500\",\"4_to_7\",3,\"male\",\"none\",3,\"real_estate\",37,\"none\",\"own\",2,\"management_self-employed\",1,\"none\",\"yes\"]\n",
    "]\n",
    "\n",
    "payload_scoring = {\"fields\": fields,\"values\": values}\n",
    "payload = {\n",
    "    wml_client.deployments.ScoringMetaNames.INPUT_DATA: [payload_scoring]\n",
    "}\n",
    "scoring_response = wml_client.deployments.score(challenger_deployment_uid, payload)\n",
    "\n",
    "scoring_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure OpenScale \n",
    "The notebook will now import the necessary libraries and set up a Python OpenScale client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_ai_openscale import APIClient4ICP\n",
    "from ibm_ai_openscale.engines import *\n",
    "from ibm_ai_openscale.utils import *\n",
    "from ibm_ai_openscale.supporting_classes import PayloadRecord, Feature\n",
    "from ibm_ai_openscale.supporting_classes.enums import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client = APIClient4ICP(aios_credentials=WOS_CREDENTIALS)\n",
    "ai_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create schema and datamart\n",
    "\n",
    "### Set up datamart\n",
    "Watson OpenScale uses a database to store payload logs and calculated metrics. If an OpenScale datamart exists in Db2, the existing datamart will be used and no data will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_mart_details = ai_client.data_mart.get_details()\n",
    "    print('Using existing external datamart')\n",
    "except:\n",
    "    print('Setting up external datamart')\n",
    "    ai_client.data_mart.setup(db_credentials=DATABASE_CREDENTIALS, schema=SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mart_details = ai_client.data_mart.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mart_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an ICP token\n",
    "\n",
    "The following is a function that will generate an ICP access token used to interact with the Watson OpenScale APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_access_token():\n",
    "    headers={}\n",
    "    headers[\"Accept\"] = \"application/json\"\n",
    "    auth = HTTPBasicAuth(WOS_CREDENTIALS[\"username\"], WOS_CREDENTIALS[\"password\"])\n",
    "    \n",
    "    ICP_TOKEN_URL= WOS_CREDENTIALS[\"url\"] + \"/v1/preauth/validateAuth\"\n",
    "    \n",
    "    response = requests.get(ICP_TOKEN_URL, headers=headers, auth=auth, verify=False)\n",
    "    json_data = response.json()\n",
    "    icp_access_token = json_data['accessToken']\n",
    "    return icp_access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind WML machine learning instance as Pre-Prod\n",
    "\n",
    "Watson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model. If a binding with name \"WML Pre-Prod\" already exists, this code will delete that binding a create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bindings = ai_client.data_mart.bindings.get_details()['service_bindings']\n",
    "for binding in all_bindings:\n",
    "    if binding['entity']['name'] == \"WML Pre-Prod\":\n",
    "        binding_uid = binding['metadata']['guid']\n",
    "        ai_client.data_mart.bindings.delete(binding_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"WML Pre-Prod\",\n",
    "    \"description\": \"WML Instance designated as Pre-Production\",\n",
    "    \"service_type\": \"watson_machine_learning\",\n",
    "    \"instance_id\": str(uuid.uuid4()),\n",
    "    \"credentials\": {\n",
    "    },\n",
    "    \"operational_space_id\": \"pre_production\",\n",
    "    \"deployment_space_id\": space_id\n",
    "}\n",
    "\n",
    "binding_uid = str(uuid.uuid4())\n",
    "\n",
    "SERVICE_BINDINGS_URL = WOS_CREDENTIALS[\"url\"] + \"/v1/data_marts/{0}/service_bindings/{1}\".format(WOS_GUID, binding_uid)\n",
    "\n",
    "response = requests.put(SERVICE_BINDINGS_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binding_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client.data_mart.bindings.get_details(binding_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an integration to IBM OpenPages\n",
    "\n",
    "To push metrics from Watson OpenScale to IBM OpenPages, a connection must be established between the two services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "INTEGRATED_SYSTEMS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/integrated_systems\".format(WOS_GUID)\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"OpenPages Connection\",\n",
    "    \"type\": \"open_pages\",\n",
    "    \"description\": \"Integration with OpenPages\",\n",
    "    \"credentials\": OPENPAGES_CREDENTIALS\n",
    "}\n",
    "\n",
    "response = requests.post(INTEGRATED_SYSTEMS_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()\n",
    "print(json_data)\n",
    "\n",
    "if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n",
    "    integrated_system_id = json_data[\"metadata\"][\"id\"]\n",
    "    print(integrated_system_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subscriptions\n",
    "### Remove existing PreProd and Challenger credit risk subscriptions\n",
    "This code removes previous subscriptions to the German Credit model to refresh the monitors with the new model and new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\n",
    "for subscription in subscriptions_uids:\n",
    "    sub_name = ai_client.data_mart.subscriptions.get_details(subscription)['entity']['asset']['name']\n",
    "    if sub_name == PRE_PROD_MODEL_NAME or sub_name == PRE_PROD_CHALLENGER_MODEL_NAME:\n",
    "        ai_client.data_mart.subscriptions.delete(subscription)\n",
    "        print('Deleted existing subscription for', sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prod_subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n",
    "    pre_prod_model_uid,\n",
    "    binding_uid=binding_uid,\n",
    "    problem_type=ProblemType.BINARY_CLASSIFICATION,\n",
    "    input_data_type=InputDataType.STRUCTURED,\n",
    "    label_column='Risk',\n",
    "    prediction_column='predictedLabel',\n",
    "    probability_column='probability',\n",
    "    feature_columns = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"],\n",
    "    categorical_columns = [\"CheckingStatus\",\"CreditHistory\",\"LoanPurpose\",\"ExistingSavings\",\"EmploymentDuration\",\"Sex\",\"OthersOnLoan\",\"OwnsProperty\",\"InstallmentPlans\",\"Housing\",\"Job\",\"Telephone\",\"ForeignWorker\"]\n",
    "))\n",
    "\n",
    "if pre_prod_subscription is None:\n",
    "    print('Subscription already exists; get the existing one')\n",
    "    subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\n",
    "    for sub in subscriptions_uids:\n",
    "        if ai_client.data_mart.subscriptions.get_details(sub)['entity']['asset']['name'] == PRE_PROD_MODEL_NAME:\n",
    "            pre_prod_subscription = ai_client.data_mart.subscriptions.get(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n",
    "    challenger_model_uid,\n",
    "    binding_uid=binding_uid,\n",
    "    problem_type=ProblemType.BINARY_CLASSIFICATION,\n",
    "    input_data_type=InputDataType.STRUCTURED,\n",
    "    label_column='Risk',\n",
    "    prediction_column='prediction',\n",
    "    probability_column='probability',\n",
    "    feature_columns = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"],\n",
    "    categorical_columns = [\"CheckingStatus\",\"CreditHistory\",\"LoanPurpose\",\"ExistingSavings\",\"EmploymentDuration\",\"Sex\",\"OthersOnLoan\",\"OwnsProperty\",\"InstallmentPlans\",\"Housing\",\"Job\",\"Telephone\",\"ForeignWorker\"]\n",
    "))\n",
    "\n",
    "if challenger_subscription is None:\n",
    "    print('Subscription already exists; get the existing one')\n",
    "    subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\n",
    "    for sub in subscriptions_uids:\n",
    "        if ai_client.data_mart.subscriptions.get_details(sub)['entity']['asset']['name'] == PRE_PROD_CHALLENGER_MODEL_NAME:\n",
    "            challenger_subscription = ai_client.data_mart.subscriptions.get(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client.data_mart.subscriptions.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prod_subscription.uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_subscription.uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch the training data reference in both the preprod & challenger subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "training_data_reference = {\n",
    "  \"connection\": {\n",
    "    \"connection_string\": \"jdbc:db2://dashdb-txn-sbox-yp-dal09-03.services.dal.bluemix.net:50000/BLUDB:retrieveMessagesFromServerOnGetMessage=true;\",\n",
    "    \"database_name\": \"BLUDB\",\n",
    "    \"hostname\": \"dashdb-txn-sbox-yp-dal09-03.services.dal.bluemix.net\",\n",
    "    \"password\": \"khhz72v+6mcwwkfv\",\n",
    "    \"username\": \"cmb91569\"\n",
    "  },\n",
    "  \"location\": {\n",
    "    \"schema_name\": \"CMB91569\",\n",
    "    \"table_name\": \"CREDIT_RISK_TRAIN_DATA\"\n",
    "  },\n",
    "  \"name\": \"German credit risk training data\",\n",
    "  \"type\": \"db2\"\n",
    "}\n",
    "\n",
    "payload = [\n",
    " {\n",
    "   \"op\": \"replace\",\n",
    "   \"path\": \"/asset_properties/training_data_reference\",\n",
    "   \"value\": training_data_reference\n",
    " }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_URL = WOS_CREDENTIALS[\"url\"] + \"/v1/data_marts/{0}/service_bindings/{1}/subscriptions/{2}\".format(WOS_GUID, binding_uid, pre_prod_subscription.uid)\n",
    "\n",
    "response = requests.patch(SUBSCRIPTION_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_URL = WOS_CREDENTIALS[\"url\"] + \"/v1/data_marts/{0}/service_bindings/{1}/subscriptions/{2}\".format(WOS_GUID, binding_uid, challenger_subscription.uid)\n",
    "\n",
    "response = requests.patch(SUBSCRIPTION_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the model so we can configure monitors\n",
    "Now that the WML service has been bound and the subscription has been created, we need to send a request to the model before we configure OpenScale. This allows OpenScale to create a payload log in the datamart with the correct schema, so it can capture data coming into and out of the model. First, the code gets the model deployment's endpoint URL, and then sends a few records for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"]\n",
    "values = [\n",
    "  [\"no_checking\",13,\"credits_paid_to_date\",\"car_new\",1343,\"100_to_500\",\"1_to_4\",2,\"female\",\"none\",3,\"savings_insurance\",46,\"none\",\"own\",2,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",24,\"prior_payments_delayed\",\"furniture\",4567,\"500_to_1000\",\"1_to_4\",4,\"male\",\"none\",4,\"savings_insurance\",36,\"none\",\"free\",2,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",26,\"all_credits_paid_back\",\"car_new\",863,\"less_100\",\"less_1\",2,\"female\",\"co-applicant\",2,\"real_estate\",38,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",14,\"no_credits\",\"car_new\",2368,\"less_100\",\"1_to_4\",3,\"female\",\"none\",3,\"real_estate\",29,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",4,\"no_credits\",\"car_new\",250,\"less_100\",\"unemployed\",2,\"female\",\"none\",3,\"real_estate\",23,\"none\",\"rent\",1,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",17,\"credits_paid_to_date\",\"car_new\",832,\"100_to_500\",\"1_to_4\",2,\"male\",\"none\",2,\"real_estate\",42,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",33,\"outstanding_credit\",\"appliances\",5696,\"unknown\",\"greater_7\",4,\"male\",\"co-applicant\",4,\"unknown\",54,\"none\",\"free\",2,\"skilled\",1,\"yes\",\"yes\"],\n",
    "  [\"0_to_200\",13,\"prior_payments_delayed\",\"retraining\",1375,\"100_to_500\",\"4_to_7\",3,\"male\",\"none\",3,\"real_estate\",37,\"none\",\"own\",2,\"management_self-employed\",1,\"none\",\"yes\"]\n",
    "]\n",
    "\n",
    "payload_scoring = {\"fields\": fields,\"values\": values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    wml_client.deployments.ScoringMetaNames.INPUT_DATA: [payload_scoring]\n",
    "}\n",
    "scoring_response = wml_client.deployments.score(pre_prod_deployment_uid, payload)\n",
    "\n",
    "print('Single record scoring result:', '\\n fields:', scoring_response['predictions'][0]['fields'], '\\n values: ', scoring_response['predictions'][0]['values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "pre_prod_subscription.payload_logging.get_records_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    wml_client.deployments.ScoringMetaNames.INPUT_DATA: [payload_scoring]\n",
    "}\n",
    "scoring_response = wml_client.deployments.score(challenger_deployment_uid, payload)\n",
    "\n",
    "print('Single record scoring result:', '\\n fields:', scoring_response['predictions'][0]['fields'], '\\n values: ', scoring_response['predictions'][0]['values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "challenger_subscription.payload_logging.get_records_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality monitoring\n",
    "\n",
    "## Enable quality monitoring\n",
    "The code below waits ten seconds to allow the payload logging table to be set up before it begins enabling monitors. First, it turns on the quality (accuracy) monitor and sets an alert threshold of 80%. OpenScale will show an alert on the dashboard if the model accuracy measurement (area under the curve, in the case of a binary classifier) falls below this threshold.\n",
    "\n",
    "The second paramater supplied, min_records, specifies the minimum number of feedback records OpenScale needs before it calculates a new measurement. The quality monitor runs hourly, but the accuracy reading in the dashboard will not change until an additional 50 feedback records have been added, via the user interface, the Python client, or the supplied feedback endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "pre_prod_subscription.quality_monitoring.enable(threshold=0.8, min_records=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "challenger_subscription.quality_monitoring.enable(threshold=0.8, min_records=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness, drift monitoring and explanations \n",
    "\n",
    "## Fairness configuration\n",
    "The code below configures fairness monitoring for our model. It turns on monitoring for two features, Sex and Age. In each case, we must specify:\n",
    "\n",
    "Which model feature to monitor\n",
    "One or more majority groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n",
    "One or more minority groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n",
    "The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 80%)\n",
    "Additionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 100 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_data = pd.read_csv(\"german_credit_data_biased_training.csv\", sep=\",\", header=0)\n",
    "\n",
    "pre_prod_subscription.fairness_monitoring.enable(\n",
    "            features=[\n",
    "                Feature(\"Sex\", majority=['male'], minority=['female'], threshold=0.80),\n",
    "                Feature(\"Age\", majority=[[26,74]], minority=[[19,25]], threshold=0.80)\n",
    "            ],\n",
    "            favourable_classes=['No Risk'],\n",
    "            unfavourable_classes=['Risk'],\n",
    "            min_records=100,\n",
    "            training_data=pd_data\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_subscription.fairness_monitoring.enable(\n",
    "            features=[\n",
    "                Feature(\"Sex\", majority=['male'], minority=['female'], threshold=0.80),\n",
    "                Feature(\"Age\", majority=[[26,74]], minority=[[19,25]], threshold=0.80)\n",
    "            ],\n",
    "            favourable_classes=['No Risk'],\n",
    "            unfavourable_classes=['Risk'],\n",
    "            min_records=100,\n",
    "            training_data=pd_data\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift configuration\n",
    "\n",
    "Enable the drift configuration for both the subscription created with a threshold of 10% and minimal sample as 100 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prod_subscription.drift_monitoring.enable(threshold=0.10, min_records=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_status = None\n",
    "while drift_status != 'finished':\n",
    "    drift_details = pre_prod_subscription.drift_monitoring.get_details()\n",
    "    drift_status = drift_details['parameters']['config_status']['state']\n",
    "    if drift_status != 'finished':\n",
    "        print(datetime.utcnow().strftime('%H:%M:%S'), drift_status)\n",
    "        time.sleep(30)\n",
    "print(drift_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_subscription.drift_monitoring.enable(threshold=0.10, min_records=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_status = None\n",
    "while drift_status != 'finished':\n",
    "    drift_details = challenger_subscription.drift_monitoring.get_details()\n",
    "    drift_status = drift_details['parameters']['config_status']['state']\n",
    "    if drift_status != 'finished':\n",
    "        print(datetime.utcnow().strftime('%H:%M:%S'), drift_status)\n",
    "        time.sleep(30)\n",
    "print(drift_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Explainability\n",
    "Finally, we provide OpenScale with the training data to enable and configure the explainability features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_ai_openscale.supporting_classes import *\n",
    "\n",
    "pre_prod_subscription.explainability.enable(training_data=pd_data)\n",
    "challenger_subscription.explainability.enable(training_data=pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable MRM \n",
    "\n",
    "We enable the MRM configuration for both the subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "payload = {\n",
    "  \"data_mart_id\": WOS_GUID,\n",
    "  \"monitor_definition_id\": \"mrm\",\n",
    "  \"target\": {\n",
    "    \"target_id\": pre_prod_subscription.uid,\n",
    "    \"target_type\": \"subscription\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "  },\n",
    "  \"managed_by\": \"user\"\n",
    "}\n",
    "\n",
    "MONITOR_INSTANCES_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances\".format(WOS_GUID)\n",
    "\n",
    "response = requests.post(MONITOR_INSTANCES_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()\n",
    "print(json_data)\n",
    "if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n",
    "    pre_prod_mrm_instance_id = json_data[\"metadata\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "payload = {\n",
    "  \"data_mart_id\": WOS_GUID,\n",
    "  \"monitor_definition_id\": \"mrm\",\n",
    "  \"target\": {\n",
    "    \"target_id\": challenger_subscription.uid,\n",
    "    \"target_type\": \"subscription\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "  },\n",
    "  \"managed_by\": \"user\"\n",
    "}\n",
    "\n",
    "MONITOR_INSTANCES_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances\".format(WOS_GUID)\n",
    "\n",
    "response = requests.post(MONITOR_INSTANCES_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()\n",
    "print(json_data)\n",
    "if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n",
    "    challenger_mrm_instance_id = json_data[\"metadata\"][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch the integration reference in the pre-prod subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "payload = [\n",
    "  {\n",
    "    \"op\": \"add\",\n",
    "    \"path\": \"/integration_reference\",\n",
    "    \"value\": {\n",
    "        \"integrated_system_id\": integrated_system_id,\n",
    "        \"external_id\": openpages_model_id\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "SUBSCRIPTIONS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/subscriptions/{1}\".format(WOS_GUID, pre_prod_subscription.uid)\n",
    "\n",
    "response = requests.patch(SUBSCRIPTIONS_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test data sets from the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = pd_data[1:201]\n",
    "test_data_1.to_csv(\"german_credit_risk_test_data_1.csv\", encoding=\"utf-8\", index=False)\n",
    "test_data_2 = pd_data[201:401]\n",
    "test_data_2.to_csv(\"german_credit_risk_test_data_2.csv\", encoding=\"utf-8\", index=False)\n",
    "test_data_3 = pd_data[401:601]\n",
    "test_data_3.to_csv(\"german_credit_risk_test_data_3.csv\", encoding=\"utf-8\", index=False)\n",
    "test_data_4 = pd_data[601:801]\n",
    "test_data_4.to_csv(\"german_credit_risk_test_data_4.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to upload, evaluate and check the status of the evaluation\n",
    "\n",
    "This function will upload the test data CSV and trigger the risk evaluation. It will iterate and check the status of the evaluation until its finished with a finite wait duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_evaluate(file_name, mrm_instance_id):\n",
    "    \n",
    "    print(\"Running upload and evaluate for {}\".format(file_name))\n",
    "    import json\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    status = None\n",
    "    monitoring_run_id = None\n",
    "    GET_UPLOAD_AND_EVALUATION_STATUS_RETRIES = 32\n",
    "    GET_UPLOAD_AND_EVALUATION_STATUS_INTERVAL = 10\n",
    "    \n",
    "    if file_name is not None:\n",
    "        \n",
    "        headers = {}\n",
    "        headers[\"Content-Type\"] = \"text/csv\"\n",
    "        headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "        \n",
    "        POST_EVALUATIONS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitoring_services/mrm/monitor_instances/{1}/risk_evaluations?test_data_set_name={2}\".format(WOS_GUID, mrm_instance_id, file_name)\n",
    "\n",
    "        with open(file_name) as file:\n",
    "            f = file.read()\n",
    "            b = bytearray(f, 'utf-8')\n",
    "\n",
    "        response = requests.post(POST_EVALUATIONS_URL, data=bytes(b), headers=headers, verify=False)\n",
    "        if response.ok is False:\n",
    "            print(\"Upload and evalaute for {0} failed with {1}: {2}\".format(file_name, response.status_code, response.reason))\n",
    "            return\n",
    "        \n",
    "        headers = {}\n",
    "        headers[\"Content-Type\"] = \"application/json\"\n",
    "        headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "        GET_EVALUATIONS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitoring_services/mrm/monitor_instances/{1}/risk_evaluations\".format(WOS_GUID, mrm_instance_id)\n",
    "        \n",
    "        for i in range(GET_UPLOAD_AND_EVALUATION_STATUS_RETRIES):\n",
    "        \n",
    "            response = requests.get(GET_EVALUATIONS_URL, headers=headers, verify=False)\n",
    "            if response.ok is False:\n",
    "                print(\"Getting status of upload and evalaute for {0} failed with {1}: {2}\".format(file_name, response.status_code, response.reason))\n",
    "                return\n",
    "\n",
    "            response = json.loads(response.text)\n",
    "            if \"metadata\" in response and \"id\" in response[\"metadata\"]:\n",
    "                monitoring_run_id = response[\"metadata\"][\"id\"]\n",
    "            if \"entity\" in response and \"status\" in response[\"entity\"]:\n",
    "                status = response[\"entity\"][\"status\"][\"state\"]\n",
    "            \n",
    "            if status is not None:\n",
    "                print(datetime.utcnow().strftime('%H:%M:%S'), status.lower())\n",
    "                if status.lower() in [\"finished\", \"completed\"]:\n",
    "                    break\n",
    "                elif \"error\" in status.lower():\n",
    "                    print(response)\n",
    "                    break\n",
    "\n",
    "            time.sleep(GET_UPLOAD_AND_EVALUATION_STATUS_INTERVAL)\n",
    "\n",
    "    return status, monitoring_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Risk Evaluations\n",
    "\n",
    "We now start performing evaluations of smaller data sets against both the PreProd and Challenger subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_1.csv\", pre_prod_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_2.csv\", pre_prod_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_3.csv\", pre_prod_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_4.csv\", pre_prod_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_1.csv\", challenger_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_2.csv\", challenger_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_3.csv\", challenger_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_evaluate(\"german_credit_risk_test_data_4.csv\", challenger_mrm_instance_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Model Risk Management UI\n",
    "\n",
    "Here is a quick recap of what we have done so far.\n",
    "\n",
    "1. We've deployed two Credit Risk Model to a WML instance that is designated as Pre-Production\n",
    "2. We've created subscriptions of these two model deployments in OpenScale\n",
    "3. Configured all monitors supported by OpenScale for these subscriptions\n",
    "4. We've performed a few risk evaluations against both these susbscription with the same set of test data\n",
    "\n",
    "Now, pleaase explore the MRM UI to visualize the results, compare the performance of models, download the evaluation report as PDF\n",
    "\n",
    "Link to OpenScale : https://CLUSTER_URL/aiopenscale/insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promote pre-production model to production \n",
    "\n",
    "After you have reviewed the evaluation results of the PreProd Vs Challenger and if you make the decision to promote the Challenger model to Production, the first thing you need to do is to deploy the model into a WML instance that is designated as Production instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to production WML instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD_MODEL_NAME=\"German Credit Risk Model - Prod\"\n",
    "PROD_DEPLOYMENT_NAME=\"German Credit Risk Model - Prod\"\n",
    "\n",
    "PROD_SPACE_NAME=\"prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list()\n",
    "\n",
    "# Find and set the default space\n",
    "space_name=PROD_SPACE_NAME\n",
    "spaces = wml_client.spaces.get_details()['resources']\n",
    "space_id = None\n",
    "for space in spaces:\n",
    "    if space['entity']['name'] == space_name:\n",
    "        space_id = space[\"metadata\"][\"guid\"]\n",
    "if space_id is None:\n",
    "    space_id = wml_client.spaces.store(\n",
    "        meta_props={wml_client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]\n",
    "wml_client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "numpy.version.version\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier,GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType, ArrayType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "pd_data = pd.read_csv(\"german_credit_data_biased_training.csv\", sep=\",\", header=0)\n",
    "spark_df = spark.read.csv(path=\"german_credit_data_biased_training.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "spark_df.head()\n",
    "\n",
    "(train_data, test_data) = spark_df.randomSplit([0.9, 0.1], 24)\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))\n",
    "\n",
    "si_CheckingStatus = StringIndexer(inputCol='CheckingStatus', outputCol='CheckingStatus_IX')\n",
    "si_CreditHistory = StringIndexer(inputCol='CreditHistory', outputCol='CreditHistory_IX')\n",
    "si_LoanPurpose = StringIndexer(inputCol='LoanPurpose', outputCol='LoanPurpose_IX')\n",
    "si_ExistingSavings = StringIndexer(inputCol='ExistingSavings', outputCol='ExistingSavings_IX')\n",
    "si_EmploymentDuration = StringIndexer(inputCol='EmploymentDuration', outputCol='EmploymentDuration_IX')\n",
    "si_Sex = StringIndexer(inputCol='Sex', outputCol='Sex_IX')\n",
    "si_OthersOnLoan = StringIndexer(inputCol='OthersOnLoan', outputCol='OthersOnLoan_IX')\n",
    "si_OwnsProperty = StringIndexer(inputCol='OwnsProperty', outputCol='OwnsProperty_IX')\n",
    "si_InstallmentPlans = StringIndexer(inputCol='InstallmentPlans', outputCol='InstallmentPlans_IX')\n",
    "si_Housing = StringIndexer(inputCol='Housing', outputCol='Housing_IX')\n",
    "si_Job = StringIndexer(inputCol='Job', outputCol='Job_IX')\n",
    "si_Telephone = StringIndexer(inputCol='Telephone', outputCol='Telephone_IX')\n",
    "si_ForeignWorker = StringIndexer(inputCol='ForeignWorker', outputCol='ForeignWorker_IX')\n",
    "si_Label = StringIndexer(inputCol=\"Risk\", outputCol=\"label\").fit(spark_df)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)\n",
    "\n",
    "va_features = VectorAssembler(\n",
    "inputCols=[\"CheckingStatus_IX\", \"CreditHistory_IX\", \"LoanPurpose_IX\", \"ExistingSavings_IX\",\n",
    "           \"EmploymentDuration_IX\", \"Sex_IX\", \"OthersOnLoan_IX\", \"OwnsProperty_IX\", \"InstallmentPlans_IX\",\n",
    "           \"Housing_IX\", \"Job_IX\", \"Telephone_IX\", \"ForeignWorker_IX\", \"LoanDuration\", \"LoanAmount\",\n",
    "           \"InstallmentPercent\", \"CurrentResidenceDuration\", \"LoanDuration\", \"Age\", \"ExistingCreditsCount\",\n",
    "           \"Dependents\"], outputCol=\"features\")\n",
    "\n",
    "classifier=GBTClassifier(featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "stages=[si_CheckingStatus, si_CreditHistory, si_EmploymentDuration, si_ExistingSavings, si_ForeignWorker,\n",
    "        si_Housing, si_InstallmentPlans, si_Job, si_LoanPurpose, si_OthersOnLoan,\n",
    "        si_OwnsProperty, si_Sex, si_Telephone, si_Label, va_features, classifier, label_converter])\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy = %g\" % auc)\n",
    "\n",
    "# Remove existing model and deployment\n",
    "MODEL_NAME=PROD_MODEL_NAME\n",
    "DEPLOYMENT_NAME=PROD_DEPLOYMENT_NAME\n",
    "\n",
    "deployment_details = wml_client.deployments.get_details()\n",
    "for deployment in deployment_details['resources']:\n",
    "    deployment_id = deployment['metadata']['guid']\n",
    "    model_id = deployment['entity']['asset']['href'].split('/')[3].split('?')[0]\n",
    "    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n",
    "        print('Deleting deployment id', deployment_id)\n",
    "        wml_client.deployments.delete(deployment_id)\n",
    "        print('Deleting model id', model_id)\n",
    "        wml_client.repository.delete(model_id)\n",
    "wml_client.repository.list_models()\n",
    "\n",
    "# Save Model\n",
    "model_props_rf = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.DESCRIPTION: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.RUNTIME_UID: \"spark-mllib_2.3\",\n",
    "    wml_client.repository.ModelMetaNames.TYPE: \"mllib_2.3\"\n",
    "}\n",
    "\n",
    "published_model_details = wml_client.repository.store_model(model=model, meta_props=model_props_rf, training_data=train_data, pipeline=pipeline)\n",
    "print(published_model_details)\n",
    "\n",
    "# List models in the repository\n",
    "wml_client.repository.list_models()\n",
    "\n",
    "# Get the model UID\n",
    "prod_model_uid = wml_client.repository.get_model_uid(published_model_details)\n",
    "prod_model_uid\n",
    "\n",
    "\n",
    "# Deploy model\n",
    "wml_deployments = wml_client.deployments.get_details()\n",
    "prod_deployment_uid = None\n",
    "for deployment in wml_deployments['resources']:\n",
    "    if DEPLOYMENT_NAME == deployment['entity']['name']:\n",
    "        prod_deployment_uid = deployment['metadata']['guid']\n",
    "        break\n",
    "\n",
    "if prod_deployment_uid is None:\n",
    "    print(\"Deploying model...\")\n",
    "    meta_props = {\n",
    "        wml_client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n",
    "        wml_client.deployments.ConfigurationMetaNames.DESCRIPTION: DEPLOYMENT_NAME,\n",
    "        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "    deployment = wml_client.deployments.create(artifact_uid=prod_model_uid, name=DEPLOYMENT_NAME, meta_props=meta_props)\n",
    "    prod_deployment_uid = wml_client.deployments.get_uid(deployment)\n",
    "\n",
    "print(\"Model id: {}\".format(prod_model_uid))\n",
    "print(\"Deployment id: {}\".format(prod_deployment_uid))\n",
    "\n",
    "prod_deployment_uid=wml_client.deployments.get_uid(deployment)\n",
    "prod_deployment_uid\n",
    "\n",
    "fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"]\n",
    "values = [\n",
    "  [\"no_checking\",13,\"credits_paid_to_date\",\"car_new\",1343,\"100_to_500\",\"1_to_4\",2,\"female\",\"none\",3,\"savings_insurance\",46,\"none\",\"own\",2,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",24,\"prior_payments_delayed\",\"furniture\",4567,\"500_to_1000\",\"1_to_4\",4,\"male\",\"none\",4,\"savings_insurance\",36,\"none\",\"free\",2,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",26,\"all_credits_paid_back\",\"car_new\",863,\"less_100\",\"less_1\",2,\"female\",\"co-applicant\",2,\"real_estate\",38,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",14,\"no_credits\",\"car_new\",2368,\"less_100\",\"1_to_4\",3,\"female\",\"none\",3,\"real_estate\",29,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",4,\"no_credits\",\"car_new\",250,\"less_100\",\"unemployed\",2,\"female\",\"none\",3,\"real_estate\",23,\"none\",\"rent\",1,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",17,\"credits_paid_to_date\",\"car_new\",832,\"100_to_500\",\"1_to_4\",2,\"male\",\"none\",2,\"real_estate\",42,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",33,\"outstanding_credit\",\"appliances\",5696,\"unknown\",\"greater_7\",4,\"male\",\"co-applicant\",4,\"unknown\",54,\"none\",\"free\",2,\"skilled\",1,\"yes\",\"yes\"],\n",
    "  [\"0_to_200\",13,\"prior_payments_delayed\",\"retraining\",1375,\"100_to_500\",\"4_to_7\",3,\"male\",\"none\",3,\"real_estate\",37,\"none\",\"own\",2,\"management_self-employed\",1,\"none\",\"yes\"]\n",
    "]\n",
    "\n",
    "payload_scoring = {\"fields\": fields,\"values\": values}\n",
    "payload = {\n",
    "    wml_client.deployments.ScoringMetaNames.INPUT_DATA: [payload_scoring]\n",
    "}\n",
    "scoring_response = wml_client.deployments.score(prod_deployment_uid, payload)\n",
    "\n",
    "print('Single record scoring result:', '\\n fields:', scoring_response['predictions'][0]['fields'], '\\n values: ', scoring_response['predictions'][0]['values'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind WML machine learning instance as Prod\n",
    "\n",
    "Watson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model. If a binding with name \"WML Prod\" already exists, this code will delete that binding a create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bindings = ai_client.data_mart.bindings.get_details()['service_bindings']\n",
    "for binding in all_bindings:\n",
    "    if binding['entity']['name'] == \"WML Prod\":\n",
    "        binding_uid = binding['metadata']['guid']\n",
    "        ai_client.data_mart.bindings.delete(binding_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"WML Prod\",\n",
    "    \"description\": \"WML Instance designated as Production\",\n",
    "    \"service_type\": \"watson_machine_learning\",\n",
    "    \"instance_id\": str(uuid.uuid4()),\n",
    "    \"credentials\": {\n",
    "    },\n",
    "    \"operational_space_id\": \"production\",\n",
    "    \"deployment_space_id\": space_id\n",
    "}\n",
    "\n",
    "binding_uid = str(uuid.uuid4())\n",
    "\n",
    "SERVICE_BINDINGS_URL = WOS_CREDENTIALS[\"url\"] + \"/v1/data_marts/{0}/service_bindings/{1}\".format(WOS_GUID, binding_uid)\n",
    "\n",
    "response = requests.put(SERVICE_BINDINGS_URL, json=payload, headers=headers, verify=False)\n",
    "json_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binding_uid)\n",
    "ai_client.data_mart.bindings.get_details(binding_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import configuration settings from pre-prod model\n",
    "\n",
    "With MRM we provide a important feature that lets you copy the configuration settings of your pre-production subscription to the production subscription. To try this out\n",
    "\n",
    "1. Navigate to Model Monitors view in Insights dashboard of OpenScale\n",
    "2. Click on the Add to dashboard\n",
    "3. Select the production model deployment from WML production machine learning provider and click on Configure\n",
    "4. In Selections saved dialog, click on Configure monitors\n",
    "4. In Model Governance, click on Begin and choose the same OpenPages model that was linked to the pre-production model and click on Save\n",
    "5. You will be presented with an dialog about the OpenPages model being associated with another subscription and if you'd like to import settings.\n",
    "6. Click on Import and confirm\n",
    "\n",
    "All the configuration settings are now copied into the production subscription\n",
    "\n",
    "\n",
    "<b>Note: The next set of cells should be executed only after finishing the import settings from the OpenScale dashboard</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the production model so that we can trigger monitors\n",
    "\n",
    "Now that the production subscription is configured by copying the configuration, there would be schedules created for each of the monitors to run on a scheduled basis. \n",
    "Quality, Fairness and Mrm will run hourly. Drift will run once in three hours.\n",
    "\n",
    "For this demo purpose, we will trigger the monitors on-demand so that we can see the model summary dashboard without having to wait the entire hour. \n",
    "To do that lets first push some records in the Payload Logging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd_data.sample(n=400)\n",
    "df = df.drop(['Risk'], axis=1)\n",
    "fields = df.columns.tolist()\n",
    "values = df.values.tolist()\n",
    "\n",
    "payload_scoring = {\"fields\": fields,\"values\": values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    wml_client.deployments.ScoringMetaNames.INPUT_DATA: [payload_scoring]\n",
    "}\n",
    "scoring_response = wml_client.deployments.score(prod_deployment_uid, payload)\n",
    "\n",
    "print('Single record scoring result:', '\\n fields:', scoring_response['predictions'][0]['fields'], '\\n values: ', scoring_response['predictions'][0]['values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_subscription = ai_client.data_mart.subscriptions.get(name=PROD_DEPLOYMENT_NAME)\n",
    "prod_subscription.uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "prod_subscription.payload_logging.get_records_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all monitor instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "MONITOR_INSTANCES_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances?target.target_id={1}&target.target_type=subscription\".format(WOS_GUID, prod_subscription.uid)\n",
    "print(MONITOR_INSTANCES_URL)\n",
    "\n",
    "response = requests.get(MONITOR_INSTANCES_URL, headers=headers, verify=False)\n",
    "monitor_instances = response.json()[\"monitor_instances\"]\n",
    "\n",
    "drift_monitor_instance_id = None\n",
    "quality_monitor_instance_id = None\n",
    "mrm_monitor_instance_id = None\n",
    "\n",
    "if monitor_instances is not None:\n",
    "    for monitor_instance in monitor_instances:\n",
    "        if \"entity\" in monitor_instance and \"monitor_definition_id\" in monitor_instance[\"entity\"]:\n",
    "            monitor_name = monitor_instance[\"entity\"][\"monitor_definition_id\"]\n",
    "            if \"metadata\" in monitor_instance and \"id\" in monitor_instance[\"metadata\"]:\n",
    "                id = monitor_instance[\"metadata\"][\"id\"]\n",
    "                if monitor_name == \"drift\":\n",
    "                    drift_monitor_instance_id = id\n",
    "                elif monitor_name == \"quality\":\n",
    "                    quality_monitor_instance_id = id\n",
    "                elif monitor_name == \"mrm\":\n",
    "                    mrm_monitor_instance_id = id\n",
    "                    \n",
    "print(\"Quality monitor instance id - {0}\".format(quality_monitor_instance_id))\n",
    "print(\"Drift monitor instance id - {0}\".format(drift_monitor_instance_id))\n",
    "print(\"MRM monitor instance id - {0}\".format(mrm_monitor_instance_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the monitoring run details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monitoring_run_details(monitor_instance_id, monitoring_run_id):\n",
    "    \n",
    "    headers = {}\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "    headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "    \n",
    "    MONITORING_RUNS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs/{2}\".format(WOS_GUID, monitor_instance_id, monitoring_run_id)\n",
    "    response = requests.get(MONITORING_RUNS_URL, headers=headers, verify=False)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on-demand Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "if quality_monitor_instance_id is not None:\n",
    "    MONITOR_RUN_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs\".format(WOS_GUID, quality_monitor_instance_id)\n",
    "    payload = {\n",
    "        \"triggered_by\": \"user\"\n",
    "    }\n",
    "    print(\"Triggering Quality computation with {}\".format(MONITOR_RUN_URL))\n",
    "    response = requests.post(MONITOR_RUN_URL, json=payload, headers=headers, verify=False)\n",
    "    json_data = response.json()\n",
    "    if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n",
    "        quality_monitoring_run_id = json_data[\"metadata\"][\"id\"]\n",
    "    print(\"Done triggering Quality computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "quality_run_status = None\n",
    "while quality_run_status != 'finished':\n",
    "    monitoring_run_details = get_monitoring_run_details(quality_monitor_instance_id, quality_monitoring_run_id)\n",
    "    quality_run_status = monitoring_run_details[\"entity\"][\"status\"][\"state\"]\n",
    "    if quality_run_status == \"error\":\n",
    "        print(monitoring_run_details)\n",
    "        break\n",
    "    if quality_run_status != 'finished':\n",
    "        print(datetime.utcnow().strftime('%H:%M:%S'), quality_run_status)\n",
    "        time.sleep(10)\n",
    "print(quality_run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on-demand Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "if drift_monitor_instance_id is not None:\n",
    "    MONITOR_RUN_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs\".format(WOS_GUID, drift_monitor_instance_id)\n",
    "    payload = {\n",
    "        \"triggered_by\": \"user\"\n",
    "    }\n",
    "    print(\"Triggering Drift computation with {}\".format(MONITOR_RUN_URL))\n",
    "    response = requests.post(MONITOR_RUN_URL, json=payload, headers=headers, verify=False)\n",
    "    json_data = response.json()\n",
    "    if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n",
    "        drift_monitoring_run_id = json_data[\"metadata\"][\"id\"]\n",
    "    print(\"Done triggering Drift computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "drift_run_status = None\n",
    "while drift_run_status != 'finished':\n",
    "    monitoring_run_details = get_monitoring_run_details(drift_monitor_instance_id, drift_monitoring_run_id)\n",
    "    drift_run_status = monitoring_run_details[\"entity\"][\"status\"][\"state\"]\n",
    "    if drift_run_status == \"error\":\n",
    "        print(monitoring_run_details)\n",
    "        break\n",
    "    if drift_run_status != 'finished':\n",
    "        print(datetime.utcnow().strftime('%H:%M:%S'), drift_run_status)\n",
    "        time.sleep(10)\n",
    "print(drift_run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on-demand Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "FAIRNESS_RUNS_URL = WOS_CREDENTIALS[\"url\"] + \"/v1/fairness_monitoring/{0}/runs\".format(prod_subscription.uid)\n",
    "payload = {\n",
    "  \"binding_id\": binding_uid,\n",
    "  \"subscription_id\": prod_subscription.uid,\n",
    "  \"deployment_id\": prod_deployment_uid,\n",
    "  \"data_mart_id\": WOS_GUID\n",
    "}\n",
    "\n",
    "print(\"Triggering Fairness computation with {}\".format(FAIRNESS_RUNS_URL))\n",
    "response = requests.post(FAIRNESS_RUNS_URL, json=payload, headers=headers, verify=False)\n",
    "jsonData = response.json()\n",
    "print(\"Done triggering Fairness computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "fairness_run_status = None\n",
    "while fairness_run_status != 'FINISHED':\n",
    "    fairness_monitoring_details = prod_subscription.fairness_monitoring.get_details()\n",
    "    fairness_run_status = fairness_monitoring_details[\"parameters\"][\"run_status\"][prod_deployment_uid][\"run_status\"]\n",
    "    if fairness_run_status == \"FINISHED WITH ERRORS\":\n",
    "        print(fairness_monitoring_details)\n",
    "        break\n",
    "    if fairness_run_status != 'FINISHED':\n",
    "        print(datetime.utcnow().strftime('%H:%M:%S'), fairness_run_status)\n",
    "        time.sleep(10)\n",
    "print(fairness_run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on-demand MRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n",
    "\n",
    "if mrm_monitor_instance_id is not None:\n",
    "    MONITOR_RUN_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs\".format(WOS_GUID, mrm_monitor_instance_id)\n",
    "    payload = {\n",
    "        \"triggered_by\": \"user\"\n",
    "    }\n",
    "    print(\"Triggering MRM computation with {}\".format(MONITOR_RUN_URL))\n",
    "    response = requests.post(MONITOR_RUN_URL, json=payload, headers=headers, verify=False)\n",
    "    json_data = response.json()\n",
    "    if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n",
    "        mrm_monitoring_run_id = json_data[\"metadata\"][\"id\"]\n",
    "    print(\"Done triggering MRM computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "mrm_run_status = None\n",
    "while mrm_run_status != 'finished':\n",
    "    monitoring_run_details = get_monitoring_run_details(mrm_monitor_instance_id, mrm_monitoring_run_id)\n",
    "    mrm_run_status = monitoring_run_details[\"entity\"][\"status\"][\"state\"]\n",
    "    if mrm_run_status == \"error\":\n",
    "        print(monitoring_run_details)\n",
    "        break\n",
    "    if mrm_run_status != 'finished':\n",
    "        print(datetime.utcnow().strftime('%H:%M:%S'), mrm_run_status)\n",
    "        time.sleep(10)\n",
    "print(mrm_run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh the model summary of the production subscription in the OpenScale dashboard\n",
    "\n",
    "This brings us to the end of this demo exercise. Thank you for trying it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
