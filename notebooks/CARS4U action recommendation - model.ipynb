{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<table style=\"border: none\" align=\"left\">\n    <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/pmservice/cars-4-you/master/static/images/logo.png\" width=\"200\" alt=\"Icon\"></th>\n       <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Business area prediction and action recommendation</b></th>\n   </tr>\n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<img align=left src=\"https://github.com/pmservice/cars-4-you/raw/master/static/images/action.png\" width=\"550\" alt=\"Icon\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Contents\n- [0. Setup](#setup)\n- [1. Introduction](#introduction)\n- [2. Load and explore data](#load)\n- [3. Create an Apache Spark machine learning model](#model)\n- [4. Store the model in the Watson Machine Learning repository](#persistence)\n- [5. Deploy the model in the IBM Cloud](#persistence)\n- [6. Score the model](#score)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**Note:** This notebook works correctly with kernel `Python 3.5 with Spark 2.1`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"setup\"></a>\n## 0. Setup\n\nIn this section please use below cell to upgrade the `watson-machine-learning-client`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Training data setup\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n- Create a Watson Machine Learning Service instance (a free plan is offered).\n- Create a Compose for PostgreSQL instance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Package installation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 78, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v48/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client) (0.9.3)\n"
                }
            ], 
            "source": "!rm -rf $PIP_BUILD\n!pip install --upgrade watson-machine-learning-client --no-cache | tail -n 1"
        }, 
        {
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v48/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client->ibm-ai-openscale) (0.9.3)\n"
                }
            ], 
            "source": "!pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1"
        }, 
        {
            "execution_count": 80, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\nRequirement already satisfied: psycopg2-binary in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s43a-142d17f4eb1f9a-8ef090487f81/.local/lib/python3.5/site-packages (2.7.6.1)\n"
                }
            ], 
            "source": "!pip install psycopg2-binary | tail -n 1"
        }, 
        {
            "source": "**Note**: Please restart the kernel (Kernel -> Restart)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"introduction\"></a>\n## 1. Introduction\n\nThis notebook defines, trains and deploys the model that recommends specific Action for unstatisfied customers.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"load\"></a>\n## 2. Load and explore data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this section you will upload training data to the database, load it as an Apache Spark DataFrame and perform a basic exploration.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Read data into Spark DataFrame from CSV file and show sample record.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2.1 Load data from git", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-12-13 09:20:23--  https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/data/car_rental_training_data.csv\nResolving github.com (github.com)... 192.30.253.112, 192.30.253.113\nConnecting to github.com (github.com)|192.30.253.112|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/notebooks/data/car_rental_training_data.csv [following]\n--2018-12-13 09:20:23--  https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/notebooks/data/car_rental_training_data.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 79518 (78K) [text/plain]\nSaving to: \u2018car_rental_training_data.csv.2\u2019\n\n100%[======================================>] 79,518      --.-K/s   in 0.004s  \n\n2018-12-13 09:20:23 (20.4 MB/s) - \u2018car_rental_training_data.csv.2\u2019 saved [79518/79518]\n\n"
                }
            ], 
            "source": "!wget https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/data/car_rental_training_data.csv"
        }, 
        {
            "execution_count": 82, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 82, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Row(ID=83, Gender='Female', Status='M', Children=2, Age=48.85, Customer_Status='Inactive', Car_Owner='Yes', Customer_Service='I thought the representative handled the initial situation badly.  The company was out of cars, with none coming in that day.  Then the representative tried to find us a car at another franchise.  There they were successful.', Satisfaction=0, Business_Area='Product: Availability/Variety/Size', Action='Free Upgrade')"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from pyspark.sql import SparkSession\nimport json\n\nspark = SparkSession.builder.getOrCreate()\ndf_data = spark.read.csv(path=\"car_rental_training_data.csv\", sep=\";\", header=True, inferSchema=True)\ndf_data.head()"
        }, 
        {
            "source": "### 2.2 Upload data to PostgreSQL database.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**TIP:** Put your service credentials here. Just copy paste content of Credentials tab from service details (IBM Cloud)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 83, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "POSTGRES_CREDENTIALS = {\n    \"db_type\": \"postgresql\",\n    \"name\": \"<put correct name here>\",\n    \"uri\": \"<put correct uri here>\"\n}"
        }, 
        {
            "execution_count": 84, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "Put your schema name here.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "SCHEMA_NAME = 'data_mart_for_wml'"
        }, 
        {
            "source": "You can use the code below to create the schema. If the schema already exists skipp this cell.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from ibm_ai_openscale.utils import create_postgres_schema\n\ncreate_postgres_schema(postgres_credentials=POSTGRES_CREDENTIALS, schema_name=SCHEMA_NAME)"
        }, 
        {
            "source": "Create training table.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "TABLE_NAME = \"CAR_RENTAL_TRAINING\""
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import psycopg2\n\nhostname = POSTGRES_CREDENTIALS['uri'].split('@')[1].split(':')[0]\nport = POSTGRES_CREDENTIALS['uri'].split('@')[1].split(':')[1].split('/')[0]\nuser = POSTGRES_CREDENTIALS['uri'].split('@')[0].split('//')[1].split(':')[0]\npassword = POSTGRES_CREDENTIALS['uri'].split('@')[0].split('//')[1].split(':')[1]\ndbname = 'compose'\nconn_string = \"host=\" + hostname + \" port=\" + port + \" dbname=\" + dbname + \" user=\" + user + \" password=\" + password\n\n\nconn = psycopg2.connect(conn_string)\nconn.autocommit = True\ncursor = conn.cursor()\ncursor.execute(\"\"\"\nCREATE TABLE {}.{}(\n    ID integer,\n    Gender text,\n    Status text,\n    Children integer,\n    Age decimal(14,6),\n    Customer_Status text,\n    Car_Owner text,\n    Customer_Service text,\n    Satisfaction integer,\n    Business_Area text,\n    Action text\n)\n\"\"\".format(SCHEMA_NAME, TABLE_NAME))\ncursor.close()\nconn.close()"
        }, 
        {
            "source": "Upload training data to created table.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "conn = psycopg2.connect(conn_string)\nconn.autocommit = True\ncursor = conn.cursor()\nwith open('car_rental_training_data.csv', 'r') as f:\n    next(f)\n    cursor.copy_from(file=f, table='{}.CAR_RENTAL_TRAINING'.format(SCHEMA_NAME), sep=';')\ncursor.close()\nconn.close()"
        }, 
        {
            "source": "Print first row in table.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(83, 'Female', 'M', 2, Decimal('48.850000'), 'Inactive', 'Yes', 'I thought the representative handled the initial situation badly.  The company was out of cars, with none coming in that day.  Then the representative tried to find us a car at another franchise.  There they were successful.', 0, 'Product: Availability/Variety/Size', 'Free Upgrade')]\n"
                }
            ], 
            "source": "conn = psycopg2.connect(conn_string)\nconn.autocommit = True\ncursor = conn.cursor()\ncursor.execute(\"\"\"\nSELECT * FROM {}.CAR_RENTAL_TRAINING LIMIT 1\n\"\"\".format(SCHEMA_NAME))\nresult = cursor.fetchall()\ncursor.close()\nconn.close()\n\nprint(result)"
        }, 
        {
            "source": "### 2.3 Explore data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 85, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- ID: integer (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Status: string (nullable = true)\n |-- Children: integer (nullable = true)\n |-- Age: double (nullable = true)\n |-- Customer_Status: string (nullable = true)\n |-- Car_Owner: string (nullable = true)\n |-- Customer_Service: string (nullable = true)\n |-- Satisfaction: integer (nullable = true)\n |-- Business_Area: string (nullable = true)\n |-- Action: string (nullable = true)\n\n"
                }
            ], 
            "source": "df_data.printSchema()"
        }, 
        {
            "source": "**Tip:** Code above can be inserted using Data menu.  You have to select `Insert SparkSession DataFrame` option.\n\n**Note:** Inserted code is modified to work with code in cells below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As you can see, the data contains eleven fields. `Action` field is the one you would like to predict using feedback data in `Customer_Service` field.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of records: 486\n"
                }
            ], 
            "source": "print(\"Number of records: \" + str(df_data.count()))"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------------------------+-----+\n|Business_area                     |count|\n+----------------------------------+-----+\n|Service: Accessibility            |26   |\n|Product: Functioning              |150  |\n|Service: Attitude                 |24   |\n|Service: Orders/Contracts         |32   |\n|Product: Availability/Variety/Size|42   |\n|Product: Pricing and Billing      |24   |\n|Product: Information              |8    |\n|Service: Knowledge                |180  |\n+----------------------------------+-----+\n\n"
                }
            ], 
            "source": "df_data.select('Business_area').groupBy('Business_area').count().show(truncate=False)"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------------------+-----+\n|Action                   |count|\n+-------------------------+-----+\n|NA                       |274  |\n|Voucher                  |42   |\n|Premium features         |30   |\n|On-demand pickup location|56   |\n|Free Upgrade             |84   |\n+-------------------------+-----+\n\n"
                }
            ], 
            "source": "df_data.select('Action').groupBy('Action').count().show(truncate=False)"
        }, 
        {
            "source": "<a id=\"model\"></a>\n## 3. Create an Apache Spark machine learning model\n\nIn this section you will learn how to:\n\n- [3.1 Prepare data for training a model](#prep)\n- [3.2 Create an Apache Spark machine learning pipeline](#pipe)\n- [3.3 Train a model](#train)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prep\"></a>\n### 3.1 Prepare data for training a model\n\nIn this subsection you will split your data into: train and test data set.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of training records: 391\nNumber of testing records : 95\n"
                }
            ], 
            "source": "train_data, test_data = df_data.randomSplit([0.8, 0.2], 24)\n\nprint(\"Number of training records: \" + str(train_data.count()))\nprint(\"Number of testing records : \" + str(test_data.count()))"
        }, 
        {
            "source": "### 3.2 Create the pipeline<a id=\"pipe\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this section you will create an Apache Spark machine learning pipeline and then train the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, HashingTF, IDF, Tokenizer, SQLTransformer\nfrom pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model, PipelineModel"
        }, 
        {
            "source": "In the following step, use the StringIndexer transformer to convert all the string fields to numeric ones.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = Tokenizer(inputCol=\"Customer_Service\", outputCol=\"words\")"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='hash')\nidf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"area_features\", minDocFreq=5)"
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "string_indexer_area = StringIndexer(inputCol=\"Business_Area\", outputCol=\"area_label\").fit(df_data)"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dt_area = DecisionTreeClassifier(labelCol='area_label', featuresCol=idf.getOutputCol() , predictionCol='prediction_area', probabilityCol='probability_area', rawPredictionCol='rawPrediction_area')"
        }, 
        {
            "source": "Finally, convert the indexed labels back to original labels.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "labelConverter = IndexToString(inputCol=\"prediction_area\", outputCol=\"predictedAreaLabel\", labels=string_indexer_area.labels)"
        }, 
        {
            "execution_count": 38, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "string_indexer_gender = StringIndexer(inputCol=\"Gender\", outputCol=\"gender_ix\")\nstring_indexer_customer_status = StringIndexer(inputCol=\"Customer_Status\", outputCol=\"customer_status_ix\")\nstring_indexer_status = StringIndexer(inputCol=\"Status\", outputCol=\"status_ix\")\nstring_indexer_owner = StringIndexer(inputCol=\"Car_Owner\", outputCol=\"owner_ix\")"
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "assembler = VectorAssembler(inputCols=[\"gender_ix\", \"customer_status_ix\", \"status_ix\", \"owner_ix\", \"Children\", \"Age\", \"Satisfaction\", idf.getOutputCol()], outputCol=\"features\")"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "string_indexer_action = StringIndexer(inputCol=\"Action\", outputCol=\"label\").fit(df_data)"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "label_action_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedActionLabel\", labels=string_indexer_action.labels)"
        }, 
        {
            "execution_count": 42, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dt_action = DecisionTreeClassifier()"
        }, 
        {
            "execution_count": 43, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "vector_assembler = VectorAssembler(inputCols=[\"gender_ix\", \"customer_status_ix\", \"status_ix\", \"owner_ix\", \"Children\", \"Age\", \"Satisfaction\", 'prediction_area'], outputCol=\"features\")"
        }, 
        {
            "execution_count": 44, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf, string_indexer_area, dt_area, labelConverter, string_indexer_gender, string_indexer_customer_status, string_indexer_status, string_indexer_action, string_indexer_owner, vector_assembler, dt_action, label_action_converter])"
        }, 
        {
            "source": "### 3.3 Train the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**Hint:** Training takes ~10 minutes to complete.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 45, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(train_data)"
        }, 
        {
            "source": "### 3.4 Evaluate the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 46, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictions = model.transform(test_data)"
        }, 
        {
            "execution_count": 47, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Accuracy = 0.789474\n"
                }
            ], 
            "source": "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(\"Accuracy = %g\" % accuracy)"
        }, 
        {
            "source": "<a id=\"persistence\"></a>\n## 4. Store the model in the repository", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "In this section you will store trained model to Watson Machine Learning repository. When model is stored some metada is optional, however we provide it to be able to configure Continuous Learning System.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 48, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "source": "We need Watson Machine Learning credentials to be able to store model in repository.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**TIP:** Put watson Machine Learning service credentials here.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "WML_CREDENTIALS = {\n  \"apikey\": \"***\",\n  \"iam_apikey_description\": \"***\",\n  \"iam_apikey_name\": \"***\",\n  \"iam_role_crn\": \"***\",\n  \"iam_serviceid_crn\": \"***\",\n  \"instance_id\": \"***\",\n  \"password\": \"***\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"***\"\n}"
        }, 
        {
            "execution_count": 87, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 88, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)"
        }, 
        {
            "execution_count": 89, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 89, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'1.0.347'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "client.version"
        }, 
        {
            "source": "### 4.2 Save the pipeline and model<a id=\"save\"></a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 90, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "TRAINING_DATA_REFERENCE = {\n \"name\": \"CARS4U training reference\",\n \"connection\": POSTGRES_CREDENTIALS,\n \"source\": {\n  \"tablename\": TABLE_NAME,\n  \"type\": \"postgres\"\n }\n}"
        }, 
        {
            "source": "Define `output_data_schema` for the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 91, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train_data_schema = train_data.schema\nlabel_field = next(f for f in train_data_schema.fields if f.name == \"Action\")\nlabel_field.metadata['values'] = string_indexer_action.labels"
        }, 
        {
            "execution_count": 92, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.types import *\n\ninput_fileds = filter(lambda f: f.name != \"Action\", train_data_schema.fields)\n\nOUTPUT_DATA_SCHEMA = StructType(list(input_fileds)). \\\n    add(\"prediction\", DoubleType(), True, {'modeling_role': 'prediction'}). \\\n    add(\"predictedActionLabel\", StringType(), True, {'modeling_role': 'decoded-target', 'values': string_indexer_action.labels}). \\\n    add(\"probability\", ArrayType(DoubleType()), True, {'modeling_role': 'probability'})"
        }, 
        {
            "source": "Define model's metadata.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_props = {\n    client.repository.ModelMetaNames.NAME: \"CARS4U - Action Recommendation Model\",\n    client.repository.ModelMetaNames.TRAINING_DATA_REFERENCE: TRAINING_DATA_REFERENCE,\n    client.repository.ModelMetaNames.EVALUATION_METHOD: \"multiclass\",\n    client.repository.ModelMetaNames.OUTPUT_DATA_SCHEMA: OUTPUT_DATA_SCHEMA.jsonValue(),\n    client.repository.ModelMetaNames.EVALUATION_METRICS: [\n        {\n           \"name\": \"accuracy\",\n           \"value\": accuracy,\n           \"threshold\": 0.7\n        }\n    ]\n}"
        }, 
        {
            "source": "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available meta names.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Store the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 94, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "published_model_details = client.repository.store_model(model=model, meta_props=model_props, training_data=train_data, pipeline=pipeline)"
        }, 
        {
            "execution_count": 95, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "67d1d82d-eeb0-488a-b403-1a8dc00db72f\n"
                }
            ], 
            "source": "model_uid = client.repository.get_model_uid(published_model_details)\nprint(model_uid)"
        }, 
        {
            "source": "<a id=\"deploy\"></a>\n## 5. Deploy model in the IBM Cloud", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "You can use following command to create online deployment in cloud.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 96, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '67d1d82d-eeb0-488a-b403-1a8dc00db72f' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='60e6fc28-bd6a-43b7-8b45-5eaedb2a7a79'\n------------------------------------------------------------------------------------------------\n\n\n"
                }
            ], 
            "source": "deployment_details = client.deployments.create(model_uid=model_uid, name='CARS4U - Area and Action Model Deployment')"
        }, 
        {
            "source": "You can use deployed model to score new data using scoring endpoint.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 97, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "https://us-south.ml.cloud.ibm.com/v3/wml_instances/4d7a90a8-9df1-45be-be13-cb54930512cf/deployments/60e6fc28-bd6a-43b7-8b45-5eaedb2a7a79/online\n"
                }
            ], 
            "source": "scoring_url = client.deployments.get_scoring_url(deployment_details)\nprint(scoring_url)"
        }, 
        {
            "source": "<a id=\"score\"></a>\n## 6. Score the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 98, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "fields = ['ID', 'Gender', 'Status', 'Children', 'Age', 'Customer_Status','Car_Owner', 'Customer_Service', 'Business_Area', 'Satisfaction']\nvalues = [3785, 'Male', 'S', 1, 17, 'Inactive', 'Yes', 'The car should have been brought to us instead of us trying to find it in the lot.', 'Product: Information', 0]"
        }, 
        {
            "execution_count": 99, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "payload_scoring = {\"fields\": fields,\"values\": [values]}\nscoring_response = client.deployments.score(scoring_url, payload_scoring)"
        }, 
        {
            "execution_count": 100, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Predicted Business Area: \"Service: Knowledge\"\nRecommended action: \"On-demand pickup location\"\n"
                }
            ], 
            "source": "area_index = scoring_response['fields'].index('predictedAreaLabel')\naction_index = scoring_response['fields'].index('predictedActionLabel')\n\nprint(\"Predicted Business Area: \" + json.dumps(scoring_response['values'][0][area_index]))\nprint(\"Recommended action: \" + json.dumps(scoring_response['values'][0][action_index]))"
        }, 
        {
            "source": "---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Authors\nLukasz Cmielowski, PhD, is an Automation Architect and Data Scientist at IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4
}